{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5249520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search_b_node(state: MainState) -> MainState:\n",
    "    try:\n",
    "        query = state[\"user_query\"]\n",
    "        search_results = semantic_search_b(query, top_k=10)\n",
    "        \n",
    "        result = [{\n",
    "            'id': item['id'],\n",
    "            'similarity': item['similarity_score'],\n",
    "            'text': item['text'],\n",
    "            'status': item['status']\n",
    "        } for item in search_results]\n",
    "        \n",
    "        if result:\n",
    "            print(\"retrieved document:\")\n",
    "            for item in result:\n",
    "                print(f\"text: {item['text']}\")\n",
    "                print(f\"similarity score: {item['similarity']}\")\n",
    "                print(f\"status: {item['status']}\")\n",
    "        \n",
    "        return {\"semantic_search_b_results\": result}\n",
    "    except Exception as e:\n",
    "        print(f\"Semantic search B failed: {e}\")\n",
    "        return {\"semantic_search_b_results\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4901571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intensity_score(state: MainState) -> MainState:\n",
    "    try:\n",
    "        user_query = state[\"user_query\"]\n",
    "        prompt = (\n",
    "            f\"Analyze sentiment and context of: {user_query}\\n\\n\"\n",
    "            \"Provide sentiment scores (pos, neg, neu) that sum to 1.0\\n\"\n",
    "            \"Context types: personal (user's feelings), general (about others), question (asking info), academic (educational)\\n\"\n",
    "            \"Personal relevance: 0.0=impersonal, 1.0=deeply personal\"\n",
    "        )\n",
    "\n",
    "        llm_response = llm_model.with_structured_output(SentimentScore).invoke(prompt)\n",
    "\n",
    "        sentiment_scores = {\n",
    "            'pos': llm_response.pos,\n",
    "            'neg': llm_response.neg, \n",
    "            'neu': llm_response.neu,\n",
    "            'context_type': llm_response.context_type,\n",
    "            'personal_relevance': llm_response.personal_relevance\n",
    "        }\n",
    "        \n",
    "        print(f\"pos: {llm_response.pos}, neg: {llm_response.neg}, neu: {llm_response.neu}\")\n",
    "        print(f\"context: {llm_response.context_type}, relevance: {llm_response.personal_relevance}\")\n",
    "        \n",
    "        return {\"intensity_score\": sentiment_scores}\n",
    "    except:\n",
    "        print(\"fail\")\n",
    "        return {\"intensity_score\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa7b241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_filter(state: MainState) -> MainState:\n",
    "    try:\n",
    "        search_results = state.get(\"semantic_search_b_results\", [])\n",
    "        \n",
    "        if not search_results:\n",
    "            print(\"no search results to filter\")\n",
    "            return {\"top_k_results\": []}\n",
    "        \n",
    "        user_query = state[\"user_query\"]\n",
    "        prompt = (\n",
    "            f\"Filter mental health search results for relevance to: {user_query}\\n\\n\"\n",
    "            \"Keep only results that directly relate to the user's mental state.\\n\"\n",
    "            \"Remove off-topic or generic content.\\n\\n\"\n",
    "            \"SEARCH RESULTS:\\n\"\n",
    "        )\n",
    "        \n",
    "        # Limit to first 5 results to avoid token limits\n",
    "        for i, result in enumerate(search_results[:5], 1):\n",
    "            prompt += (\n",
    "                f\"{i}. ID: {result['id']}\\n\"\n",
    "                f\"   Similarity: {result['similarity']:.3f}\\n\"\n",
    "                f\"   Status: {result['status']}\\n\"\n",
    "                f\"   Text: {result['text'][:200]}...\\n\\n\"\n",
    "            )\n",
    "        \n",
    "        llm_response = llm_model.with_structured_output(List[FilteredResult]).invoke(prompt)\n",
    "        \n",
    "        filtered_results = [\n",
    "            {\n",
    "                \"id\": result.id,\n",
    "                \"similarity\": result.similarity,\n",
    "                \"status\": result.status,\n",
    "                \"text\": result.text\n",
    "            }\n",
    "            for result in llm_response\n",
    "        ]\n",
    "        \n",
    "        print(f\"filtered {len(filtered_results)} from {len(search_results)} results\")\n",
    "        \n",
    "        return {\"top_k_results\": filtered_results}\n",
    "    except Exception as e:\n",
    "        print(\"filter fail\")\n",
    "        return {\"top_k_results\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bb91cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_path_B(state: MainState) -> MainState:\n",
    "    intensity_score = state.get(\"intensity_score\", {})\n",
    "    top_k_results = state.get(\"top_k_results\", [])\n",
    "    \n",
    "    # Check what data we have\n",
    "    if intensity_score and top_k_results:\n",
    "        print(\"merged!\")\n",
    "    elif not intensity_score and not top_k_results:\n",
    "        print(\"intensity-score: null\")\n",
    "        print(\"top_k: null\")\n",
    "    elif not intensity_score:\n",
    "        print(\"intensity-score: null\")\n",
    "    elif not top_k_results:\n",
    "        print(\"top_k: null\")\n",
    "    \n",
    "    # Simple passthrough - no new fields needed\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "550ff81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculator_func(state: MainState) -> MainState:\n",
    "    try:\n",
    "        top_k_results = state.get(\"top_k_results\", [])\n",
    "        \n",
    "        # Fix field names to match calculator expectations\n",
    "        fixed_top_k = []\n",
    "        for result in top_k_results:\n",
    "            fixed_result = {\n",
    "                \"similarity_score\": result[\"similarity\"],  # Fix field name\n",
    "                \"label\": result[\"status\"],                 # Fix field name  \n",
    "                \"text\": result[\"text\"],\n",
    "                \"id\": result[\"id\"]\n",
    "            }\n",
    "            fixed_top_k.append(fixed_result)\n",
    "        \n",
    "        intensity_score = state.get(\"intensity_score\", {\n",
    "            \"pos\": 0.33, \"neg\": 0.33, \"neu\": 0.34,\n",
    "            \"context_type\": \"personal\", \"personal_relevance\": 1.0\n",
    "        })\n",
    "        \n",
    "        current_scores = state.get(\"user_scores\")\n",
    "        decay_scores = state.get(\"user_decay_scores\") \n",
    "        last_update = state.get(\"last_update_timestamp\")\n",
    "        \n",
    "        # Calculate updated scores\n",
    "        updated_scores, updated_decay, timestamp, calc_result = calculator.calculate_scores(\n",
    "            user_id=\"temp\",  # Not used in calculation logic\n",
    "            top_k_results=fixed_top_k,\n",
    "            intensity_score=intensity_score,\n",
    "            current_scores=current_scores,\n",
    "            decay_scores=decay_scores,\n",
    "            last_update_timestamp=last_update\n",
    "        )\n",
    "        \n",
    "        # Validate calc_result\n",
    "        calc_result = max(0.0, min(100.0, calc_result))\n",
    "        \n",
    "        print(f\"calc_result: {calc_result:.2f}\")\n",
    "        \n",
    "        return {\n",
    "            \"user_scores\": updated_scores,\n",
    "            \"user_decay_scores\": updated_decay,\n",
    "            \"last_update_timestamp\": timestamp,\n",
    "            \"calc_result\": calc_result\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"calculator fail: {e}\")\n",
    "        return {\n",
    "            \"user_scores\": state.get(\"user_scores\", {}),\n",
    "            \"user_decay_scores\": state.get(\"user_decay_scores\", {}),\n",
    "            \"last_update_timestamp\": state.get(\"last_update_timestamp\"),\n",
    "            \"calc_result\": 0.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c7482c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warning_gen_flag(state: MainState) -> MainState:\n",
    "    try:\n",
    "        calc_result = state.get(\"calc_result\", 0.0)\n",
    "        user_scores = state.get(\"user_scores\", {})\n",
    "        \n",
    "        if not user_scores:\n",
    "            print(\"no warning - no scores\")\n",
    "            return {\"warning_text\": \"\"}\n",
    "        \n",
    "        # Check for critical conditions first (suicide risk)\n",
    "        suicidal_score = user_scores.get(\"Suicidal\", 0.0)\n",
    "        if suicidal_score > 70.0:  # High suicide risk regardless of calc_result\n",
    "            print(f\"critical warning - suicidal: {suicidal_score:.1f}\")\n",
    "            return {\"warning_text\": f\"Critical concern detected: Suicidal indicators ({suicidal_score:.1f}/100)\"}\n",
    "        \n",
    "        # Standard threshold-based warning (lowered to 30 for better sensitivity)\n",
    "        if calc_result >= 30.0:\n",
    "            # Get concerning scores (excluding Normal, above 15/100)\n",
    "            negative_scores = [\n",
    "                (label, score) for label, score in user_scores.items() \n",
    "                if label != \"Normal\" and score > 15.0\n",
    "            ]\n",
    "            \n",
    "            # Sort by highest scores and get top 2\n",
    "            top_2 = sorted(negative_scores, key=lambda x: x[1], reverse=True)[:2]\n",
    "            \n",
    "            if len(top_2) >= 2:\n",
    "                warning_text = f\"Elevated indicators: {top_2[0][0]} ({top_2[0][1]:.1f}/100), {top_2[1][0]} ({top_2[1][1]:.1f}/100)\"\n",
    "            elif len(top_2) == 1:\n",
    "                warning_text = f\"Elevated indicators: {top_2[0][0]} ({top_2[0][1]:.1f}/100)\"\n",
    "            else:\n",
    "                warning_text = \"General concern detected\"\n",
    "            \n",
    "            print(f\"warning generated - calc: {calc_result:.1f}\")\n",
    "            return {\"warning_text\": warning_text}\n",
    "        \n",
    "        print(\"no warning generated\")\n",
    "        return {\"warning_text\": \"\"}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"warning fail: {e}\")\n",
    "        return {\"warning_text\": \"\"}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
